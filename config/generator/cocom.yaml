init_args: 
    _target_: models.generators.llm_cocom.LLMCocom
    model_name: "cocom"
    batch_size: 64
    decoder_model_name: 'mistralai/Mistral-7B-Instruct-v0.2'
    checkpoint_path: null
    context_max_length: 128
    max_new_tokens: 128
    model_max_length: 1280
    compr_rate: null
    compr_model_name: null
    compr_n_layers: null # only useful for 'mistral_trimmed' compr_model_name
    quantization: 'no'
    save_generated_embeddings_path: null