{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3103056c-ab52-4258-8051-c31f3819ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f58a453-26ce-4b9f-a1aa-ed04f806d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"naver/processed-ambigqa\", token=\"put_your_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf43227-81f1-460e-ba70-35febe9c538a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['nq_id', 'question', 'nq_answer', 'disambiguated_questions', 'disambiguated_answers', 'input_passages'],\n",
       "        num_rows: 1069\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c307f447-07b0-4e1c-811c-edfe932a0e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4910574660426743103', '5706779148132378909', '82822746343941622', '-3775683158491019675', '-5333049627570569397', '5398964400508507960', '-7609206244947636807', '-4995398567532229571', '-2886061734189281128', '3886101581671349079', '7652576904728829439', '-196855123397981737', '-256357645980686898', '3152896460331899719', '651294105754681460', '3127369420834732535', '-6912552847579078268', '-6098775182466144557', '-91977775899598632', '1261841418358932814', '-1233203944270329571', '-7896024025216455001', '-5091579308296294034', '-648842070307266365', '7133553793803549181', '-6335401608477926483', '-4286108617234818000', '1040777011551693796', '4249908079104820528', '-6491913195208307840', '2547299592085156135', '-5333576035638611676', '-2046820450216612197', '1884293144811705689', '-9218899977338994404', '6061459584791759224', '-426004910266087839', '3863117877494111972', '-1382665261163735407', '7494804303418493420']\n"
     ]
    }
   ],
   "source": [
    "# small sample of ids to iterate on various prompting strategies\n",
    "ids_ = dataset[\"test\"][\"nq_id\"]\n",
    "random.shuffle(ids_)\n",
    "ids_ = ids_[:40]\n",
    "print(ids_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eaf2da2-5b28-4e9f-a57a-e27d1470c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT:\n",
      " You are an expert at writing precise detailed instructions for language models. Your sole duty is to write instructions that can be used for precisely evaluating the ability of various retrieval models to follow such instructions. Answer succinctly and carefully, and follow all instructions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# is it really needed? maybe we can have something better\n",
    "# the system prompt is used when sending data to GPT (not directly in the prompt)\n",
    "\n",
    "system_prompt = \"You are an expert at writing precise detailed instructions for language models. Your sole duty is to write instructions that can be used for precisely evaluating the ability of various retrieval models to follow such instructions. Answer succinctly and carefully, and follow all instructions.\\n\"\n",
    "print(\"SYSTEM PROMPT:\\n\", system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196bee9a-cb3e-4ea5-9b52-4a9e684ea158",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V1 prompt\n",
    "### contrasting queries\n",
    "(version used for the hackathon)\n",
    "TODO  \n",
    "- [x] change the output format (no need for the original query). \n",
    "- [ ] can be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8049dea-836a-4ad3-bafb-3e888fd9e282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Input data  \n",
      "I have an original query that is naturally ambiguous. I also have {} queries that have been disambiguated ({}). \n",
      "original query: {}\n",
      "{}\n",
      "## Your task\n",
      "I need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to {} but not relevant to {}. Provide detailed specifics in the instruction for what makes a document relevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. Output the response in JSON form only with no other text, with the key: “instruction” (str).\n",
      "## Your output (JSON only):\n"
     ]
    }
   ],
   "source": [
    "template_prompt_basic = \"## Input data  \\nI have an original query that is naturally ambiguous. I also have {} queries that have been disambiguated ({}). \\noriginal query: {}\\n{}\\n## Your task\\nI need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to {} but not relevant to {}. Provide detailed specifics in the instruction for what makes a document relevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. Output the response in JSON form only with no other text, with the key: “instruction” (str).\\n## Your output (JSON only):\"\n",
    "print(template_prompt_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "46d4d2ca-e705-4f20-b4b3-57cfac1045ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template(query_ori, queries, template):\n",
    "    \"\"\"fill template prompt w/ input queries\n",
    "    here, simplification: we contrast the **first** disambiguated query to the other ones\n",
    "    later we might change\n",
    "    \"\"\"\n",
    "    n = len(queries)\n",
    "    query_list = [f\"query_{i}\" for i in range(1, n+1)]\n",
    "    return template.format(n, \n",
    "                           \", \".join(query_list), \n",
    "                           query_ori, \n",
    "                           \"\\n\".join([f\"query_{i}: {queries[i-1]}\" for i in range(1, n+1)]), \n",
    "                           \"query_1\", \n",
    "                           \", \".join(query_list[1:]))\n",
    "\n",
    "def build_gpt_input(data, template, filter_ids=None):\n",
    "    out = {}\n",
    "    for item in data:\n",
    "        if (filter_ids is None) or (item[\"nq_id\"] in filter_ids):\n",
    "            queries = item[\"disambiguated_questions\"]\n",
    "            queries = queries[:6]  # to simplify, only consider six queries max\n",
    "            query_ori = item[\"question\"]\n",
    "            out[\"{}_{}\".format(item[\"nq_id\"], 1)] = fill_template(query_ori, queries, template)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d42d9f0-8a88-4149-8e94-bb46c33f692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_basic = build_gpt_input(dataset[\"test\"], template_prompt_basic, ids_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b6d7c95-fc8d-40e4-bd23-cc3fb575729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Input data  \n",
      "I have an original query that is naturally ambiguous. I also have 3 queries that have been disambiguated (query_1, query_2, query_3). \n",
      "original query: Who opened the gate in the prison walking dead?\n",
      "query_1: From the viewer's persepctive, who opened the gate to the prison, at the beginning of the walking dead's \"Killer Within\" episode?\n",
      "query_2: Who is the character that is discovered to have opened the gate in the prison walking dead?\n",
      "query_3: Who is the actor that is discovered to have opened the gate in the prison walking dead?\n",
      "## Your task\n",
      "I need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to query_1 but not relevant to query_2, query_3. Provide detailed specifics in the instruction for what makes a document relevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. Output the response in JSON form only with no other text, with the key: “instruction” (str).\n",
      "## Your output (JSON only):\n"
     ]
    }
   ],
   "source": [
    "print(out_basic[\"4910574660426743103_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d88b48fc-0e83-48a6-b2ac-0ae56ad2a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(out_basic, open(\"data/prompt_samples/prompt_v1.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a6ef5-9df3-4f00-946e-48e1a55fe9a4",
   "metadata": {},
   "source": [
    "# V2 prompt\n",
    "### contrasting queries after estimating passage relevance.  \n",
    "version used for the hackathon (the basic form, without the passage filtering).  \n",
    "TODO  \n",
    "- [x] change the output format (no need for the original query).\n",
    "- [ ] can be improved\n",
    "- [ ] maybe check the prompt from RankGPT?\n",
    "- [ ] filter passages that actually contain the target answer? make the life easier for the LLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da34cb87-1de8-4eee-86dd-17d75ea97239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Input Data  \n",
      "I have an original query that is naturally ambiguous. I also have {} queries that have been disambiguated ({}). In addition, I have a list of ten documents which should contain the answers to each query.\n",
      "original query: {}\n",
      "{}\n",
      "{}\n",
      "## Your task\n",
      "I need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to {} but not relevant to {}. You first need to identify which document is most relevant to {} (if any), before generating the corresponding instruction. Provide detailed specifics in the instruction for what makes this specific document relevant. Remember that this criteria should make the one document relevant and all others irrelevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. If no document is judged relevant, output “null“ in the corresponding “relevant_document_id” field. Output the response in JSON form only with no other text, with the keys: “instruction” (str) and “relevant_document_id” (str).\n",
      "## Your output (JSON only):\n"
     ]
    }
   ],
   "source": [
    "template_prompt_basic_with_docs = \"## Input Data  \\nI have an original query that is naturally ambiguous. I also have {} queries that have been disambiguated ({}). In addition, I have a list of ten documents which should contain the answers to each query.\\noriginal query: {}\\n{}\\n{}\\n## Your task\\nI need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to {} but not relevant to {}. You first need to identify which document is most relevant to {} (if any), before generating the corresponding instruction. Provide detailed specifics in the instruction for what makes this specific document relevant. Remember that this criteria should make the one document relevant and all others irrelevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. If no document is judged relevant, output “null“ in the corresponding “relevant_document_id” field. Output the response in JSON form only with no other text, with the keys: “instruction” (str) and “relevant_document_id” (str).\\n## Your output (JSON only):\"\n",
    "print(template_prompt_basic_with_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "199443cc-4f05-4629-aeed-5f27eed47060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template_with_docs(query_ori, queries, docs, template):\n",
    "    \"\"\"here, simplification: we contrast the **first** disambiguated query to the other ones\n",
    "    \"\"\"\n",
    "    n = len(queries)\n",
    "    query_list = [f\"query_{i}\" for i in range(1, n+1)]\n",
    "    return template.format(n, \n",
    "                           \", \".join(query_list), \n",
    "                           query_ori, \n",
    "                           \"\\n\".join([f\"query_{i}: {queries[i-1]}\" for i in range(1, n+1)]), \n",
    "                           \"\\n\".join([f\"document_{i}: {docs[i-1]}\" for i in range(1, len(docs) + 1)]),\n",
    "                           \"query_1\", \n",
    "                           \", \".join(query_list[1:]),\n",
    "                           \"query_1\"\n",
    "                          )\n",
    "\n",
    "def build_gpt_input_with_docs(data, template, filter_ids=None):\n",
    "    out = {}\n",
    "    for item in data:\n",
    "        if (filter_ids is None) or (item[\"nq_id\"] in filter_ids):\n",
    "            queries = item[\"disambiguated_questions\"]\n",
    "            queries = queries[:6]  # to simplify, only consider six queries max\n",
    "            query_ori = item[\"question\"]\n",
    "            docs = item[\"input_passages\"]\n",
    "            out[\"{}_{}\".format(item[\"nq_id\"], 1)] = fill_template_with_docs(query_ori, queries, docs, template)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b1dab06-4277-4bae-9b08-396c020b5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_with_docs = build_gpt_input_with_docs(dataset[\"test\"], template_prompt_basic_with_docs, ids_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1371ced-3339-4c9e-a8ae-3b1ec72b1788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Input Data  \n",
      "I have an original query that is naturally ambiguous. I also have 3 queries that have been disambiguated (query_1, query_2, query_3). In addition, I have a list of ten documents which should contain the answers to each query.\n",
      "original query: Who opened the gate in the prison walking dead?\n",
      "query_1: From the viewer's persepctive, who opened the gate to the prison, at the beginning of the walking dead's \"Killer Within\" episode?\n",
      "query_2: Who is the character that is discovered to have opened the gate in the prison walking dead?\n",
      "query_3: Who is the actor that is discovered to have opened the gate in the prison walking dead?\n",
      "document_1: # The Prisoners (The Walking Dead)\n",
      "been better off just leaving once he managed to get the gate open Bex Schwartz wrote in her review for Rolling Stone magazine that when Andrew the tiny prisoner tries to get Oscar to shoot Rick Oscar shoots Andrew instead because Oscar understands life and death and remembers that Andrew was one of the bad dudes Moore commented on how Andrew died in the series It s weird because I have so much fun playing bad guys and I loved the way Andrew went out He went out like a G and he took a few people with him Die\n",
      "document_2: # The Prisoners (The Walking Dead)\n",
      "The Hollywood Reporter noted that in Killer Within Rick s decision to lock Andrew out amid a sea of walkers came back to bite the group in a major way Glen Mazzara felt that this decision would haunt Rick as Rick believed he was committing an act of murder to save the group and that murder led to deaths within his own group and forced his own son to put down his mother Los Angeles Times columnist Laura Hudson considered the identity of the mysterious figure breaking open the lock on the prison gates to be not that much of\n",
      "document_3: # Killer Within\n",
      "the program Killer Within was viewed by 9 27 million viewers including 4 9 percent of those in the 18 to 49 demographic upon its initial broadcast in the United States Plot Outside the prison an unidentified individual lures a group of walkers towards the prison breaking open the front gate s lock to allow them inside Woodbury Michonne Danai Gurira is suspicious of how The Governor David Morrissey recovered supplies from a nearby National Guard camp but he refuses to discuss it She expresses her concerns with Andrea Laurie Holden and believes they should leave and head for the\n",
      "document_4: # Too Far Gone (The Walking Dead)\n",
      "the group at the prison face imminent danger as The Governor David Morrissey and his forces suddenly close in Hershel Greene and Michonne are held captive as The Governor demands to take hold of the prison despite the urges of the group to stay This episode marks the conclusion to the prison story arc in the television series as well as the conflict with The Governor which had begun in the third season Commentators lauded the episode while many particularly praised the climactic ending deaths the demolition of the prison and the performances particularly from Andrew Lincoln David Morrissey and\n",
      "document_5: # The Prisoners (The Walking Dead)\n",
      "prisoners and asks Rick to let them join the group but Rick is adamant about keeping to their earlier compromise Later a horde of walkers is let into the prison courtyard resulting in the deaths of T Dog and Lori When the prison s sirens sound off Oscar explains that the back up generators are powering the alarms and takes Rick to shut them down Andrew who survived and is revealed to have let the walkers loose and turned on the alarms attacks Rick in the generator room Oscar kills Andrew and Oscar and Axel are then allowed to stay\n",
      "document_6: # Too Far Gone (The Walking Dead)\n",
      "Lilly near a river bank where Meghan gets attacked by a walker and is killed At the prison the remaining sick are recuperating Daryl Dixon Norman Reedus is upset at Rick Andrew Lincoln for exiling Carol after learning she killed Karen and David and the two decide to tell Tyreese Chad L Coleman about her involvement in the murders Tyreese discovers a dissected rabbit and believes whoever did it was the killer Rick tells him otherwise but before he can explain the three hear a large explosion signaling The Governor s arrival While Daryl formulates an escape plan The Governor\n",
      "document_7: # The Prisoners (The Walking Dead)\n",
      "lends insight into how the episode amounts to a particularly poignant if also problematic entry in the show s run Pigeon also notes that the opening segments of the episode establish the origins for the ensuing chaos while going to painstaking lengths to conceal the identity of the man that caused it HitFix writer Alan Sepinwall commented on Andrew s sabotage of the prison on the one hand it helps justify Rick s decision to chase after the little guy in the first place On the other it seemed like an overly elaborate plan from someone who probably would have\n",
      "document_8: # Too Far Gone (The Walking Dead)\n",
      "gives Rick an ultimatum to leave the prison or be killed using Michonne and Hershel as leverage Rick tries to reason with The Governor who becomes frustrated and holds Michonne s katana to Hershel s neck Rick appeals to the militia claiming that everyone isn t too far gone and they could still peacefully live together and resolve their differences echoing Hershel s statements The Governor calls Rick a liar and swings Michonne s sword at Hershel s neck partially decapitating him and prompting Rick and the prison inhabitants to open fire Michonne taking advantage of the situation rolls away\n",
      "document_9: # Killer Within\n",
      "gathers as Hershel Greene Scott Wilson takes his first steps after the amputation of his foot Suddenly walkers swarm the courtyard scattering the group T Dog IronE Singleton races after Carol Melissa McBride and sacrifices himself by barreling into a walker horde to let her escape Rick Glenn Steven Yeun and Daryl Norman Reedus find the gate s lock broken and re secure it to prevent further walkers from entering Rick accuses Axel and Oscar of the treachery but an alarm begins sounding due to activation of a backup generator potentially drawing more walkers Rick realizes the prisoners cannot be\n",
      "document_10: # The Prisoners (The Walking Dead)\n",
      "resulting in heavy casualties By the time Rick and his group discovered the prison it was discovered that there were only four survivors left who were locked up inside the cafeteria The two groups come into conflict after one of them Thomas Richards kills two of the survivors Rachel and Susie Greene Hershel Greene s two youngest twin daughters Dexter is falsely accused of the crime due to him being the only prisoner who was arrested for murder it is later revealed that Thomas was arrested for murder as well and initially lied about being arrested for committing tax fraud\n",
      "## Your task\n",
      "I need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to query_1 but not relevant to query_2, query_3. You first need to identify which document is most relevant to query_1 (if any), before generating the corresponding instruction. Provide detailed specifics in the instruction for what makes this specific document relevant. Remember that this criteria should make the one document relevant and all others irrelevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. If no document is judged relevant, output “null“ in the corresponding “relevant_document_id” field. Output the response in JSON form only with no other text, with the keys: “instruction” (str) and “relevant_document_id” (str).\n",
      "## Your output (JSON only):\n"
     ]
    }
   ],
   "source": [
    "print(out_with_docs[\"4910574660426743103_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0258f1e-7ca3-48d3-8809-78bbd48ce468",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(out_with_docs, open(\"data/prompt_samples/prompt_v2.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8cd80-b4f3-48b2-b0aa-746c13287aa5",
   "metadata": {},
   "source": [
    "# V3 prompt\n",
    "### constrasting with answers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb49aa1d-b57a-4330-82f3-6142ac338f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Input data  \n",
      "I have an original query that is naturally ambiguous. I also have a query that has been disambiguated. \n",
      "original query: {}\n",
      "{}\n",
      "## Your task\n",
      "I need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to the disambiguated query, but not relevant to the following topics: {}. Provide detailed specifics in the instruction for what makes a document relevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. Output the response in JSON form only with no other text, with the key: “instruction” (str).\n",
      "## Your output (JSON only):\n"
     ]
    }
   ],
   "source": [
    "template_prompt_answers = \"## Input data  \\nI have an original query that is naturally ambiguous. I also have a query that has been disambiguated. \\noriginal query: {}\\n{}\\n## Your task\\nI need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to the disambiguated query, but not relevant to the following topics: {}. Provide detailed specifics in the instruction for what makes a document relevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. Output the response in JSON form only with no other text, with the key: “instruction” (str).\\n## Your output (JSON only):\"\n",
    "print(template_prompt_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64ce83db-d56a-4b69-aea3-55346466f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template_answers(query_ori, query, answers, template):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return template.format(\n",
    "                           query_ori, \n",
    "                           f\"disambiguated query : {query}\", \n",
    "                           \", \".join(answers))\n",
    "\n",
    "def build_gpt_input_answers(data, template, filter_ids=None):\n",
    "    out = {}\n",
    "    for item in data:\n",
    "        if (filter_ids is None) or (item[\"nq_id\"] in filter_ids):\n",
    "            query = item[\"disambiguated_questions\"][0]\n",
    "            query_ori = item[\"question\"]\n",
    "            answers = item[\"disambiguated_answers\"][1:]\n",
    "            out[\"{}_{}\".format(item[\"nq_id\"], 1)] = fill_template_answers(query_ori, query, answers, template)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe40b193-2b33-47de-8c37-7ccc20edf2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_with_answers = build_gpt_input_answers(dataset[\"test\"], template_prompt_answers, ids_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc0c4b7d-286f-4cf5-a15a-0fb3596b5bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Input data  \n",
      "I have an original query that is naturally ambiguous. I also have a query that has been disambiguated. \n",
      "original query: Who opened the gate in the prison walking dead?\n",
      "disambiguated query : From the viewer's persepctive, who opened the gate to the prison, at the beginning of the walking dead's \"Killer Within\" episode?\n",
      "## Your task\n",
      "I need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to the disambiguated query, but not relevant to the following topics: Andrew, Markice Moore. Provide detailed specifics in the instruction for what makes a document relevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. Output the response in JSON form only with no other text, with the key: “instruction” (str).\n",
      "## Your output (JSON only):\n"
     ]
    }
   ],
   "source": [
    "print(out_with_answers[\"4910574660426743103_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40450596-bc7f-4030-b25b-321d91456f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(out_with_answers, open(\"data/prompts_samples/prompt_v3.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9e90c-1784-42a4-8102-b42c4f61fb76",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V4 prompt\n",
    "### constrasting with queries and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b919276f-5ee6-4a4f-9e2b-93e5d1ab36b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Input data  \n",
      "I have an original query that is naturally ambiguous. I also have {} queries that have been disambiguated ({}). \n",
      "original query: {}\n",
      "{}\n",
      "## Your task\n",
      "I need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to {} (whose answer is unknown) but not relevant to {} (whose answers are given). Provide detailed specifics in the instruction for what makes a document relevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. Output the response in JSON form only with no other text, with the key: “instruction” (str).\n",
      "## Your output (JSON only):\n"
     ]
    }
   ],
   "source": [
    "template_prompt_queries_answers = \"## Input data  \\nI have an original query that is naturally ambiguous. I also have {} queries that have been disambiguated ({}). \\noriginal query: {}\\n{}\\n## Your task\\nI need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to {} (whose answer is unknown) but not relevant to {} (whose answers are given). Provide detailed specifics in the instruction for what makes a document relevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. Output the response in JSON form only with no other text, with the key: “instruction” (str).\\n## Your output (JSON only):\"\n",
    "print(template_prompt_queries_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8fb3eeeb-681c-4435-b087-5ee8e0c79da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template_queries_answers(query_ori, queries, answers, template):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n = len(queries)\n",
    "    query_list = [f\"query_{i}\" for i in range(1, n+1)]\n",
    "    return template.format(n, \n",
    "                           \", \".join(query_list), \n",
    "                           query_ori, \n",
    "                           \"\\n\".join([f\"query_{i}: {queries[i-1]} => answer: {answers[i-1]}\" for i in range(1, n+1)]), \n",
    "                           \"query_1\", \n",
    "                           \", \".join(query_list[1:]))\n",
    "\n",
    "def build_gpt_input_queries_answers(data, template, filter_ids=None):\n",
    "    out = {}\n",
    "    for item in data:\n",
    "        if (filter_ids is None) or (item[\"nq_id\"] in filter_ids):\n",
    "            queries = item[\"disambiguated_questions\"]\n",
    "            queries = queries[:6]  # to simplify, only consider six queries max\n",
    "            query_ori = item[\"question\"]\n",
    "            answers = item[\"disambiguated_answers\"]\n",
    "            answers[0] = \"unknown\"\n",
    "            out[\"{}_{}\".format(item[\"nq_id\"], 1)] = fill_template_queries_answers(query_ori, queries, answers, template_prompt_queries_answers)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eacac1cc-343b-4ddc-89d0-bd10de809cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_with_queries_answers = build_gpt_input_queries_answers(dataset[\"test\"], fill_template_queries_answers, ids_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7429f6c8-3616-46ca-aa3c-b9b775763fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Input data  \n",
      "I have an original query that is naturally ambiguous. I also have 3 queries that have been disambiguated (query_1, query_2, query_3). \n",
      "original query: Who opened the gate in the prison walking dead?\n",
      "query_1: From the viewer's persepctive, who opened the gate to the prison, at the beginning of the walking dead's \"Killer Within\" episode? => answer: unknown\n",
      "query_2: Who is the character that is discovered to have opened the gate in the prison walking dead? => answer: Andrew\n",
      "query_3: Who is the actor that is discovered to have opened the gate in the prison walking dead? => answer: Markice Moore\n",
      "## Your task\n",
      "I need you to come up with an instruction that can be appended to the end of the original query, to help distinguish documents that are relevant to query_1 (whose answer is unknown) but not relevant to query_2, query_3 (whose answers are given). Provide detailed specifics in the instruction for what makes a document relevant. This additional instruction should provide a test for strong frontier language models to determine if they can follow instructions. Also, be very sure that the instruction is generic and does not contain the answer to the query. Output the response in JSON form only with no other text, with the key: “instruction” (str).\n",
      "## Your output (JSON only):\n"
     ]
    }
   ],
   "source": [
    "print(out_with_queries_answers[\"4910574660426743103_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c931d16d-2c9c-492f-939d-1af81ae98514",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(out_with_queries_answers, open(\"data/prompts_samples/prompt_v4.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008bc1c-5ab7-46ff-a07a-8725bc2fefcc",
   "metadata": {},
   "source": [
    "# V5 prompt\n",
    "### ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e7a28-dc9c-4e00-af28-756334835f31",
   "metadata": {},
   "source": [
    "# post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5fcfacb8-6f50-41c5-992f-dd7926bdb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(output_file, data, with_docs=False):\n",
    "    \"\"\"messy post process gpt output to csv\n",
    "    \"\"\"\n",
    "    file_name = output_file.split(\"/\")[-1].split(\".\")[0]\n",
    "    output = json.load(open(output_file))\n",
    "    nq_id = []\n",
    "    ori_query = []\n",
    "    disamb_query = []\n",
    "    disamb_answer = []\n",
    "    inst = []\n",
    "    if with_docs:\n",
    "        docs = []\n",
    "    for elem in output:\n",
    "        id_ = list(elem.keys())[0]\n",
    "        d = elem[id_]\n",
    "        if len(d) > 0:\n",
    "            id_ = id_.split(\"_\")[0]\n",
    "            nq_id.append(id_)\n",
    "            line_in_data = data.filter(lambda example: example[\"nq_id\"].startswith(id_))\n",
    "            ori_query.append(line_in_data[\"question\"][0])\n",
    "            disamb_query.append(line_in_data[\"disambiguated_questions\"][0][0])\n",
    "            disamb_answer.append(line_in_data[\"disambiguated_answers\"][0][0])\n",
    "            inst.append(d[\"instruction\"])\n",
    "            if with_docs:\n",
    "                docs.append(d[\"relevant_document_id\"])\n",
    "        else:\n",
    "            # failed gpt generation\n",
    "            print(id_)\n",
    "            print(d)\n",
    "    d = {\"nq id\": nq_id, \n",
    "         \"original query\": ori_query,\n",
    "         \"disambiguated query\": disamb_query,\n",
    "         \"disambiguated answer\": disamb_answer,\n",
    "         \"instruction\": inst}\n",
    "    if with_docs:\n",
    "        d[\"relevant_document_id\"] = docs\n",
    "    df = pd.DataFrame.from_dict(d)\n",
    "    df.to_csv(f'data/prompts_samples/instructions/{file_name}.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db913bbf-9852-42b5-9f3b-b011f801538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_process(\"/beegfs/scratch/user/hdejean/bergen_branches_test/TAR/scripts/prompt_v2_gpt4.json\",\n",
    "             dataset[\"test\"], with_docs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
